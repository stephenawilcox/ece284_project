{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fb7e9e9",
   "metadata": {},
   "source": [
    "# Arrhythmic Heartbeat Classification Model\n",
    "## ECE 284 Project - Spring 2025\n",
    "## Stephen Wilcox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9547aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --user wfdb numpy scipy matplotlib scikit-learn torch torchvision torchaudio biosppy peakutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "397ba035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wfdb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from biosppy.signals import ecg\n",
    "from collections import Counter\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1959f0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEAT_DURATION = 1.0  #seconds\n",
    "DATA_DIR = '../data'\n",
    "MITDB_PATH = os.path.join(DATA_DIR, 'mitdb')\n",
    "NSRDB_PATH = os.path.join(DATA_DIR, 'nsrdb')\n",
    "\n",
    "\n",
    "def extract_beat(signal, r_loc, beat_len):\n",
    "    half_len = beat_len // 2\n",
    "    if r_loc - half_len < 0 or r_loc + half_len > len(signal):\n",
    "        return None\n",
    "    return signal[r_loc - half_len: r_loc + half_len]\n",
    "\n",
    "def process_mitdb_record(record_name):\n",
    "    record_path = os.path.join(MITDB_PATH, record_name)\n",
    "    record = wfdb.rdrecord(record_path)\n",
    "    ann = wfdb.rdann(record_path, 'atr')\n",
    "    signal = record.p_signal[:, 0]\n",
    "    fs = record.fs\n",
    "    beat_len = int(BEAT_DURATION * fs)\n",
    "\n",
    "    beats = []\n",
    "    labels = []\n",
    "    for i, r_loc in enumerate(ann.sample):\n",
    "        beat = extract_beat(signal, r_loc, beat_len)\n",
    "        if beat is not None:\n",
    "            beats.append(beat)\n",
    "            labels.append(ann.symbol[i])\n",
    "    return beats, labels\n",
    "\n",
    "def process_nsrdb_record(record_name, segment_duration=3600):\n",
    "    record_path = os.path.join(NSRDB_PATH, record_name)\n",
    "    record = wfdb.rdrecord(record_path)\n",
    "    signal = record.p_signal[:, 0]\n",
    "    fs = record.fs\n",
    "\n",
    "    segment_samples = int(segment_duration * fs)\n",
    "    if segment_samples > len(signal):\n",
    "        segment_samples = len(signal)\n",
    "\n",
    "    segment_signal = signal[:segment_samples]\n",
    "\n",
    "    # Run ECG peak detection on the 1-hour segment only\n",
    "    ts, filtered, rpeaks, *_ = ecg.ecg(signal=segment_signal, sampling_rate=fs, show=False)\n",
    "\n",
    "    beats = []\n",
    "    labels = []\n",
    "\n",
    "    for r in rpeaks:\n",
    "        beat = extract_beat(filtered, r, beat_len=128)\n",
    "        if beat is not None:\n",
    "            beats.append(beat)\n",
    "            labels.append('N')  # assume normal for NSRDB\n",
    "    print(f\"Processed NSRDB record {record_name}, beats found: {len(beats)}\")\n",
    "    return beats, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9de9662b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSED_DATA_PATH = os.path.join(DATA_DIR, 'processed_beats_f.npz')\n",
    "\n",
    "def upsample_list(data, target_len):\n",
    "    original_indices = np.linspace(0, 1, num=len(data))\n",
    "    target_indices = np.linspace(0, 1, num=target_len)\n",
    "    return np.interp(target_indices, original_indices, data)\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    if os.path.exists(PROCESSED_DATA_PATH):\n",
    "        print(\"Loading preprocessed data...\")\n",
    "        data = np.load(PROCESSED_DATA_PATH, allow_pickle=True)\n",
    "        return data['X_scaled'], data['y'], data['label_map'].item()\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    print(\"Processing MIT-BIH...\")\n",
    "    mit_records = [f[:-4] for f in os.listdir(MITDB_PATH) if f.endswith('.dat')]\n",
    "    for rec in mit_records:\n",
    "        beats, labels = process_mitdb_record(rec)\n",
    "        X.extend(beats)\n",
    "        y.extend(labels)\n",
    "\n",
    "    print(\"Total beats extracted:\", len(X))\n",
    "\n",
    "    print(\"Processing NSRDB...\")\n",
    "    nsr_records = [f[:-4] for f in os.listdir(NSRDB_PATH) if f.endswith('.dat')]\n",
    "    for rec in nsr_records:\n",
    "        beats, labels = process_nsrdb_record(rec, segment_duration=3600)\n",
    "        for ind in range(len(beats)):\n",
    "            beats[ind] = upsample_list(beats[ind], 360)\n",
    "        X.extend(beats)\n",
    "        y.extend(labels)\n",
    "\n",
    "    print(\"Total beats extracted:\", len(X))\n",
    "\n",
    "    # Only keep labels that are in the AAMI beat mapping\n",
    "    #https://pmc.ncbi.nlm.nih.gov/articles/PMC4897569/#:~:text=The%20AAMI%20convention%20is%20used,ventricular%20contraction%20(PVC)%20and%20ventricular\n",
    "    aami_mapping = {\n",
    "        'N': 'N', 'L': 'N', 'R': 'N', 'e': 'N', 'j': 'N',\n",
    "        'A': 'S', 'a': 'S', 'J': 'S', 'S': 'S',\n",
    "        'V': 'V', 'E': 'V',\n",
    "        'F': 'F',\n",
    "        '/': 'Q', 'f': 'Q', 'Q': 'Q', '|': 'Q'\n",
    "    }\n",
    "\n",
    "    # Filter and map labels during loading\n",
    "    X_filtered = []\n",
    "    y_filtered = []\n",
    "\n",
    "    for xi, yi in zip(X, y):\n",
    "        if yi in aami_mapping:\n",
    "            X_filtered.append(xi)\n",
    "            y_filtered.append(aami_mapping[yi])\n",
    "\n",
    "    X = X_filtered\n",
    "    y = y_filtered\n",
    "\n",
    "    X_scaled = X\n",
    "\n",
    "    # Map labels to integers\n",
    "    label_set = sorted(set(y))\n",
    "    label_map = {lbl: i for i, lbl in enumerate(label_set)}\n",
    "    y_int = np.array([label_map[lbl] for lbl in y])\n",
    "\n",
    "    # Save processed data\n",
    "    np.savez_compressed(PROCESSED_DATA_PATH, X_scaled=X_scaled, y=y_int, label_map=label_map)\n",
    "    print(f\"Saved preprocessed data to {PROCESSED_DATA_PATH}\")\n",
    "    \n",
    "    return X_scaled, y_int, label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e77714d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed data...\n",
      "Class distribution: Counter({1: 179111, 2: 8170, 4: 7235, 3: 2781, 0: 802})\n",
      "{'F': 0, 'N': 1, 'Q': 2, 'S': 3, 'V': 4}\n"
     ]
    }
   ],
   "source": [
    "# Load or process data\n",
    "X_scaled, y, label_map = load_data()\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, stratify=y, random_state=69\n",
    ")\n",
    "\n",
    "print(\"Class distribution:\", Counter(y))\n",
    "print(label_map)\n",
    "\n",
    "class ECGDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        #self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.X = torch.tensor(np.array(X), dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Create datasets and loaders\n",
    "train_dataset = ECGDataset(X_train, y_train)\n",
    "test_dataset = ECGDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08236092",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(channels // reduction, channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "class ECGClassifier(nn.Module):\n",
    "    def __init__(self, input_length, num_classes):\n",
    "        super(ECGClassifier, self).__init__()\n",
    "\n",
    "        def conv_se_block(in_channels, out_channels, kernel_size):\n",
    "            layers = [\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                nn.ReLU(),\n",
    "                SEBlock(out_channels)\n",
    "            ]\n",
    "            return nn.Sequential(*layers)\n",
    "\n",
    "        # ----- Block 1 -----\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, kernel_size=20, stride=1),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(32, 64, kernel_size=20, stride=1),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(64, 128, kernel_size=20, stride=1),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            conv_se_block(128, 256, kernel_size=20),\n",
    "            nn.AdaptiveMaxPool1d(1)  # GlobalMaxPooling1D\n",
    "        )\n",
    "\n",
    "        # ----- Block 2 -----\n",
    "        def sep_conv_block(in_ch, out_ch):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv1d(in_ch, in_ch, kernel_size=3, stride=1, padding=1, groups=in_ch),  # depthwise\n",
    "                nn.Conv1d(in_ch, out_ch, kernel_size=1),  # pointwise\n",
    "                nn.ELU()\n",
    "            )\n",
    "\n",
    "        block2_layers = []\n",
    "        channels = [1, 64, 64, 128, 128, 256, 256, 512, 512]\n",
    "        for i in range(8):\n",
    "            block2_layers.append(sep_conv_block(channels[i], channels[i+1]))\n",
    "            block2_layers.append(nn.AvgPool1d(2))\n",
    "            block2_layers.append(nn.Dropout(0.2))\n",
    "        self.block2 = nn.Sequential(*block2_layers)\n",
    "        self.block2_gru = nn.GRU(input_size=512, hidden_size=64, batch_first=True)\n",
    "\n",
    "        # ----- Block 3 -----\n",
    "        block3_layers = []\n",
    "        channels = [1, 64, 64, 128, 128, 256, 256, 512, 512]\n",
    "        for i in range(8):\n",
    "            block3_layers.append(nn.Conv1d(channels[i], channels[i+1], kernel_size=3, stride=1, padding=1))\n",
    "            block3_layers.append(nn.ELU())\n",
    "            block3_layers.append(nn.MaxPool1d(2))\n",
    "            block3_layers.append(nn.Dropout(0.2))\n",
    "        self.block3 = nn.Sequential(*block3_layers)\n",
    "        self.block3_lstm = nn.LSTM(input_size=512, hidden_size=64, batch_first=True)\n",
    "\n",
    "        # ----- Block 4 -----\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Conv1d(1, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(0.2),\n",
    "            sep_conv_block(64, 128),\n",
    "            nn.AvgPool1d(2),\n",
    "            nn.Dropout(0.2),\n",
    "            sep_conv_block(128, 256),\n",
    "            nn.AvgPool1d(2),\n",
    "            nn.Dropout(0.2),\n",
    "            sep_conv_block(256, 512),\n",
    "            nn.AvgPool1d(2),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.AdaptiveAvgPool1d(1)  # GlobalAvgPooling1D\n",
    "        )\n",
    "\n",
    "        # ----- Block 5 -----\n",
    "        self.block5 = nn.Sequential(\n",
    "            conv_se_block(1, 64, kernel_size=10),\n",
    "            conv_se_block(64, 128, kernel_size=20),\n",
    "            conv_se_block(128, 256, kernel_size=20),\n",
    "            conv_se_block(256, 512, kernel_size=20),\n",
    "            nn.AdaptiveMaxPool1d(1)  # GlobalMaxPooling1D\n",
    "        )\n",
    "\n",
    "        # ----- Block 6 -----\n",
    "        self.block6 = nn.Sequential(\n",
    "            nn.Linear(64 + 64 + 512 + 256 + 512, 2048),  # GRU + LSTM + block4 global pool + block1 global pool + block 5\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Reshape to (batch, channels=1, sequence_length)\n",
    "        x = x.unsqueeze(1)\n",
    "\n",
    "        # ----- Block 1 -----\n",
    "        out1 = self.block1(x).squeeze(-1)  # shape: (B, 128)\n",
    "\n",
    "        # ----- Block 2 -----\n",
    "        out2 = self.block2(x)\n",
    "        out2 = out2.permute(0, 2, 1)  # for GRU: (B, T, C)\n",
    "        _, h_gru = self.block2_gru(out2)\n",
    "        out2 = h_gru.squeeze(0)  # shape: (B, 64)\n",
    "\n",
    "        # ----- Block 3 -----\n",
    "        out3 = self.block3(x)\n",
    "        out3 = out3.permute(0, 2, 1)  # for LSTM: (B, T, C)\n",
    "        _, (h_lstm, _) = self.block3_lstm(out3)\n",
    "        out3 = h_lstm.squeeze(0)  # shape: (B, 64)\n",
    "\n",
    "        # ----- Block 4 -----\n",
    "        out4 = self.block4(x).squeeze(-1)  # shape: (B, 512)\n",
    "\n",
    "        out5 = self.block5(x).squeeze(-1)\n",
    "\n",
    "        # ----- Concatenate -----\n",
    "        combined = torch.cat([out1, out2, out3, out4, out5], dim=1)\n",
    "\n",
    "        # ----- Block 5 -----\n",
    "        out = self.block6(combined)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32dbcf27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ECGClassifier(\n",
       "  (block1): Sequential(\n",
       "    (0): Conv1d(1, 32, kernel_size=(20,), stride=(1,))\n",
       "    (1): ELU(alpha=1.0)\n",
       "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv1d(32, 64, kernel_size=(20,), stride=(1,))\n",
       "    (4): ELU(alpha=1.0)\n",
       "    (5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv1d(64, 128, kernel_size=(20,), stride=(1,))\n",
       "    (7): ELU(alpha=1.0)\n",
       "    (8): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): Sequential(\n",
       "      (0): Conv1d(128, 256, kernel_size=(20,), stride=(1,))\n",
       "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): SEBlock(\n",
       "        (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
       "        (fc): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=16, bias=False)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=16, out_features=256, bias=False)\n",
       "          (3): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): AdaptiveMaxPool1d(output_size=1)\n",
       "  )\n",
       "  (block2): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (1): Conv1d(1, 64, kernel_size=(1,), stride=(1,))\n",
       "      (2): ELU(alpha=1.0)\n",
       "    )\n",
       "    (1): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Sequential(\n",
       "      (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "      (1): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "      (2): ELU(alpha=1.0)\n",
       "    )\n",
       "    (4): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "    (5): Dropout(p=0.2, inplace=False)\n",
       "    (6): Sequential(\n",
       "      (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "      (1): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
       "      (2): ELU(alpha=1.0)\n",
       "    )\n",
       "    (7): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "    (8): Dropout(p=0.2, inplace=False)\n",
       "    (9): Sequential(\n",
       "      (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), groups=128)\n",
       "      (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "      (2): ELU(alpha=1.0)\n",
       "    )\n",
       "    (10): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "    (11): Dropout(p=0.2, inplace=False)\n",
       "    (12): Sequential(\n",
       "      (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), groups=128)\n",
       "      (1): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
       "      (2): ELU(alpha=1.0)\n",
       "    )\n",
       "    (13): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "    (14): Dropout(p=0.2, inplace=False)\n",
       "    (15): Sequential(\n",
       "      (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), groups=256)\n",
       "      (1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "      (2): ELU(alpha=1.0)\n",
       "    )\n",
       "    (16): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "    (17): Dropout(p=0.2, inplace=False)\n",
       "    (18): Sequential(\n",
       "      (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), groups=256)\n",
       "      (1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
       "      (2): ELU(alpha=1.0)\n",
       "    )\n",
       "    (19): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "    (20): Dropout(p=0.2, inplace=False)\n",
       "    (21): Sequential(\n",
       "      (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512)\n",
       "      (1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "      (2): ELU(alpha=1.0)\n",
       "    )\n",
       "    (22): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "    (23): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (block2_gru): GRU(512, 64, batch_first=True)\n",
       "  (block3): Sequential(\n",
       "    (0): Conv1d(1, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): ELU(alpha=1.0)\n",
       "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Dropout(p=0.2, inplace=False)\n",
       "    (4): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (5): ELU(alpha=1.0)\n",
       "    (6): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Dropout(p=0.2, inplace=False)\n",
       "    (8): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (9): ELU(alpha=1.0)\n",
       "    (10): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (11): Dropout(p=0.2, inplace=False)\n",
       "    (12): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (13): ELU(alpha=1.0)\n",
       "    (14): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (15): Dropout(p=0.2, inplace=False)\n",
       "    (16): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (17): ELU(alpha=1.0)\n",
       "    (18): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (19): Dropout(p=0.2, inplace=False)\n",
       "    (20): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (21): ELU(alpha=1.0)\n",
       "    (22): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (23): Dropout(p=0.2, inplace=False)\n",
       "    (24): Conv1d(256, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (25): ELU(alpha=1.0)\n",
       "    (26): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (27): Dropout(p=0.2, inplace=False)\n",
       "    (28): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (29): ELU(alpha=1.0)\n",
       "    (30): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (31): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (block3_lstm): LSTM(512, 64, batch_first=True)\n",
       "  (block4): Sequential(\n",
       "    (0): Conv1d(1, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): ELU(alpha=1.0)\n",
       "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Dropout(p=0.2, inplace=False)\n",
       "    (4): Sequential(\n",
       "      (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "      (1): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
       "      (2): ELU(alpha=1.0)\n",
       "    )\n",
       "    (5): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "    (6): Dropout(p=0.2, inplace=False)\n",
       "    (7): Sequential(\n",
       "      (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), groups=128)\n",
       "      (1): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
       "      (2): ELU(alpha=1.0)\n",
       "    )\n",
       "    (8): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "    (9): Dropout(p=0.2, inplace=False)\n",
       "    (10): Sequential(\n",
       "      (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), groups=256)\n",
       "      (1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
       "      (2): ELU(alpha=1.0)\n",
       "    )\n",
       "    (11): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "    (12): Dropout(p=0.2, inplace=False)\n",
       "    (13): AdaptiveAvgPool1d(output_size=1)\n",
       "  )\n",
       "  (block5): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv1d(1, 64, kernel_size=(10,), stride=(1,))\n",
       "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): SEBlock(\n",
       "        (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
       "        (fc): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=4, bias=False)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=4, out_features=64, bias=False)\n",
       "          (3): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv1d(64, 128, kernel_size=(20,), stride=(1,))\n",
       "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): SEBlock(\n",
       "        (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
       "        (fc): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=8, bias=False)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8, out_features=128, bias=False)\n",
       "          (3): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv1d(128, 256, kernel_size=(20,), stride=(1,))\n",
       "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): SEBlock(\n",
       "        (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
       "        (fc): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=16, bias=False)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=16, out_features=256, bias=False)\n",
       "          (3): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv1d(256, 512, kernel_size=(20,), stride=(1,))\n",
       "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): SEBlock(\n",
       "        (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
       "        (fc): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=32, bias=False)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=32, out_features=512, bias=False)\n",
       "          (3): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): AdaptiveMaxPool1d(output_size=1)\n",
       "  )\n",
       "  (block6): Sequential(\n",
       "    (0): Linear(in_features=1408, out_features=2048, bias=True)\n",
       "    (1): ELU(alpha=1.0)\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "    (4): ELU(alpha=1.0)\n",
       "    (5): Dropout(p=0.2, inplace=False)\n",
       "    (6): Linear(in_features=1024, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_length = X_scaled.shape[1]\n",
    "num_classes = len(label_map)\n",
    "# print(label_map)\n",
    "print(input_length)\n",
    "\n",
    "model = ECGClassifier(input_length=input_length, num_classes=num_classes)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "294726bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.088307, best loss: inf, time: 186.703\n",
      "Checkpoint saved.\n",
      "Epoch 2/100, Loss: 0.037183, best loss: 0.088307, time: 189.057\n",
      "Checkpoint saved.\n",
      "Epoch 3/100, Loss: 0.030479, best loss: 0.037183, time: 190.409\n",
      "Checkpoint saved.\n",
      "Epoch 4/100, Loss: 0.026987, best loss: 0.030479, time: 189.703\n",
      "Checkpoint saved.\n",
      "Epoch 5/100, Loss: 0.023124, best loss: 0.026987, time: 189.911\n",
      "Checkpoint saved.\n",
      "Epoch 6/100, Loss: 0.021264, best loss: 0.023124, time: 190.179\n",
      "Checkpoint saved.\n",
      "Epoch 7/100, Loss: 0.019498, best loss: 0.021264, time: 189.850\n",
      "Checkpoint saved.\n",
      "Epoch 8/100, Loss: 0.017677, best loss: 0.019498, time: 189.543\n",
      "Checkpoint saved.\n",
      "Epoch 9/100, Loss: 0.016085, best loss: 0.017677, time: 189.120\n",
      "Checkpoint saved.\n",
      "Epoch 10/100, Loss: 0.015191, best loss: 0.016085, time: 188.426\n",
      "Checkpoint saved.\n",
      "Epoch 11/100, Loss: 0.014101, best loss: 0.015191, time: 187.894\n",
      "Checkpoint saved.\n",
      "Epoch 12/100, Loss: 0.012513, best loss: 0.014101, time: 187.361\n",
      "Checkpoint saved.\n",
      "Epoch 13/100, Loss: 0.012690, best loss: 0.012513, time: 186.901\n",
      "Epoch 14/100, Loss: 0.011493, best loss: 0.012513, time: 184.079\n",
      "Checkpoint saved.\n",
      "Epoch 15/100, Loss: 0.011075, best loss: 0.011493, time: 185.502\n",
      "Checkpoint saved.\n",
      "Epoch 16/100, Loss: 0.010412, best loss: 0.011075, time: 186.986\n",
      "Checkpoint saved.\n",
      "Epoch 17/100, Loss: 0.010415, best loss: 0.010412, time: 186.367\n",
      "Epoch 18/100, Loss: 0.009306, best loss: 0.010412, time: 186.375\n",
      "Checkpoint saved.\n",
      "Epoch 19/100, Loss: 0.008188, best loss: 0.009306, time: 186.201\n",
      "Checkpoint saved.\n",
      "Epoch 20/100, Loss: 0.008767, best loss: 0.008188, time: 185.678\n",
      "Epoch 21/100, Loss: 0.003432, best loss: 0.008188, time: 185.749\n",
      "Checkpoint saved.\n",
      "Epoch 22/100, Loss: 0.002352, best loss: 0.003432, time: 185.605\n",
      "Checkpoint saved.\n",
      "Epoch 23/100, Loss: 0.001935, best loss: 0.002352, time: 185.171\n",
      "Checkpoint saved.\n",
      "Epoch 24/100, Loss: 0.001644, best loss: 0.001935, time: 184.634\n",
      "Checkpoint saved.\n",
      "Epoch 25/100, Loss: 0.001399, best loss: 0.001644, time: 182.954\n",
      "Checkpoint saved.\n",
      "Epoch 26/100, Loss: 0.001287, best loss: 0.001399, time: 183.992\n",
      "Checkpoint saved.\n",
      "Epoch 27/100, Loss: 0.001191, best loss: 0.001287, time: 183.149\n",
      "Checkpoint saved.\n",
      "Epoch 28/100, Loss: 0.001108, best loss: 0.001191, time: 183.048\n",
      "Checkpoint saved.\n",
      "Epoch 29/100, Loss: 0.001004, best loss: 0.001108, time: 182.583\n",
      "Checkpoint saved.\n",
      "Epoch 30/100, Loss: 0.001065, best loss: 0.001004, time: 182.595\n",
      "Epoch 31/100, Loss: 0.000844, best loss: 0.001004, time: 182.925\n",
      "Checkpoint saved.\n",
      "Epoch 32/100, Loss: 0.000875, best loss: 0.000844, time: 182.375\n",
      "Epoch 33/100, Loss: 0.000852, best loss: 0.000844, time: 182.655\n",
      "Epoch 34/100, Loss: 0.000865, best loss: 0.000844, time: 182.732\n",
      "Epoch 35/100, Loss: 0.000779, best loss: 0.000844, time: 182.389\n",
      "Checkpoint saved.\n",
      "Epoch 36/100, Loss: 0.000771, best loss: 0.000779, time: 183.594\n",
      "Checkpoint saved.\n",
      "Epoch 37/100, Loss: 0.000664, best loss: 0.000771, time: 184.216\n",
      "Checkpoint saved.\n",
      "Epoch 38/100, Loss: 0.000633, best loss: 0.000664, time: 185.197\n",
      "Checkpoint saved.\n",
      "Epoch 39/100, Loss: 0.000584, best loss: 0.000633, time: 185.292\n",
      "Checkpoint saved.\n",
      "Epoch 40/100, Loss: 0.000671, best loss: 0.000584, time: 184.550\n",
      "Epoch 41/100, Loss: 0.000350, best loss: 0.000584, time: 184.744\n",
      "Checkpoint saved.\n",
      "Epoch 42/100, Loss: 0.000329, best loss: 0.000350, time: 185.281\n",
      "Checkpoint saved.\n",
      "Epoch 43/100, Loss: 0.000287, best loss: 0.000329, time: 184.708\n",
      "Checkpoint saved.\n",
      "Epoch 44/100, Loss: 0.000288, best loss: 0.000287, time: 184.756\n",
      "Epoch 45/100, Loss: 0.000271, best loss: 0.000287, time: 185.566\n",
      "Checkpoint saved.\n",
      "Epoch 46/100, Loss: 0.000291, best loss: 0.000271, time: 184.323\n",
      "Epoch 47/100, Loss: 0.000260, best loss: 0.000271, time: 183.193\n",
      "Checkpoint saved.\n",
      "Epoch 48/100, Loss: 0.000249, best loss: 0.000260, time: 183.313\n",
      "Checkpoint saved.\n",
      "Epoch 49/100, Loss: 0.000252, best loss: 0.000249, time: 183.843\n",
      "Epoch 50/100, Loss: 0.000232, best loss: 0.000249, time: 184.607\n",
      "Checkpoint saved.\n",
      "Epoch 51/100, Loss: 0.000254, best loss: 0.000232, time: 183.920\n",
      "Epoch 52/100, Loss: 0.000199, best loss: 0.000232, time: 184.610\n",
      "Checkpoint saved.\n",
      "Epoch 53/100, Loss: 0.000230, best loss: 0.000199, time: 182.856\n",
      "Epoch 54/100, Loss: 0.000222, best loss: 0.000199, time: 182.715\n",
      "Epoch 55/100, Loss: 0.000223, best loss: 0.000199, time: 182.533\n",
      "Epoch 56/100, Loss: 0.000231, best loss: 0.000199, time: 182.881\n",
      "Epoch 57/100, Loss: 0.000193, best loss: 0.000199, time: 184.197\n",
      "Checkpoint saved.\n",
      "Epoch 58/100, Loss: 0.000200, best loss: 0.000193, time: 183.162\n",
      "Epoch 59/100, Loss: 0.000204, best loss: 0.000193, time: 183.239\n",
      "Epoch 60/100, Loss: 0.000193, best loss: 0.000193, time: 183.141\n",
      "Checkpoint saved.\n",
      "Epoch 61/100, Loss: 0.000199, best loss: 0.000193, time: 183.086\n",
      "Epoch 62/100, Loss: 0.000154, best loss: 0.000193, time: 183.171\n",
      "Checkpoint saved.\n",
      "Epoch 63/100, Loss: 0.000160, best loss: 0.000154, time: 183.031\n",
      "Epoch 64/100, Loss: 0.000185, best loss: 0.000154, time: 183.214\n",
      "Epoch 65/100, Loss: 0.000179, best loss: 0.000154, time: 183.416\n",
      "Epoch 66/100, Loss: 0.000167, best loss: 0.000154, time: 183.786\n",
      "Epoch 67/100, Loss: 0.000170, best loss: 0.000154, time: 183.571\n",
      "Epoch 68/100, Loss: 0.000161, best loss: 0.000154, time: 184.074\n",
      "Epoch 69/100, Loss: 0.000150, best loss: 0.000154, time: 183.757\n",
      "Checkpoint saved.\n",
      "Epoch 70/100, Loss: 0.000149, best loss: 0.000150, time: 183.023\n",
      "Checkpoint saved.\n",
      "Epoch 71/100, Loss: 0.000156, best loss: 0.000149, time: 181.197\n",
      "Epoch 72/100, Loss: 0.000163, best loss: 0.000149, time: 182.293\n",
      "Epoch 73/100, Loss: 0.000166, best loss: 0.000149, time: 182.479\n",
      "Epoch 74/100, Loss: 0.000167, best loss: 0.000149, time: 183.730\n",
      "Epoch 75/100, Loss: 0.000181, best loss: 0.000149, time: 184.303\n",
      "Epoch 76/100, Loss: 0.000164, best loss: 0.000149, time: 184.302\n",
      "Epoch 77/100, Loss: 0.000171, best loss: 0.000149, time: 183.927\n",
      "Epoch 78/100, Loss: 0.000161, best loss: 0.000149, time: 183.547\n",
      "Epoch 79/100, Loss: 0.000158, best loss: 0.000149, time: 183.414\n",
      "Epoch 80/100, Loss: 0.000181, best loss: 0.000149, time: 184.199\n",
      "Epoch 81/100, Loss: 0.000184, best loss: 0.000149, time: 183.339\n",
      "Epoch 82/100, Loss: 0.000139, best loss: 0.000149, time: 183.612\n",
      "Checkpoint saved.\n",
      "Epoch 83/100, Loss: 0.000155, best loss: 0.000139, time: 182.596\n",
      "Epoch 84/100, Loss: 0.000171, best loss: 0.000139, time: 182.797\n",
      "Epoch 85/100, Loss: 0.000167, best loss: 0.000139, time: 183.304\n",
      "Epoch 86/100, Loss: 0.000177, best loss: 0.000139, time: 183.521\n",
      "Epoch 87/100, Loss: 0.000152, best loss: 0.000139, time: 182.647\n",
      "Epoch 88/100, Loss: 0.000170, best loss: 0.000139, time: 182.540\n",
      "Epoch 89/100, Loss: 0.000160, best loss: 0.000139, time: 182.990\n",
      "Epoch 90/100, Loss: 0.000174, best loss: 0.000139, time: 183.652\n",
      "Epoch 91/100, Loss: 0.000169, best loss: 0.000139, time: 183.494\n",
      "Epoch 92/100, Loss: 0.000189, best loss: 0.000139, time: 183.715\n",
      "Epoch 93/100, Loss: 0.000158, best loss: 0.000139, time: 183.435\n",
      "Epoch 94/100, Loss: 0.000163, best loss: 0.000139, time: 183.831\n",
      "Epoch 95/100, Loss: 0.000173, best loss: 0.000139, time: 183.644\n",
      "Epoch 96/100, Loss: 0.000157, best loss: 0.000139, time: 183.922\n",
      "Epoch 97/100, Loss: 0.000161, best loss: 0.000139, time: 183.771\n",
      "Epoch 98/100, Loss: 0.000156, best loss: 0.000139, time: 183.540\n",
      "Epoch 99/100, Loss: 0.000176, best loss: 0.000139, time: 183.687\n",
      "Epoch 100/100, Loss: 0.000167, best loss: 0.000139, time: 183.543\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = os.path.join(DATA_DIR, 'ecg_checkpoint_fupgcnn_hcrnet_6_7.pth')\n",
    "start_epoch = 0\n",
    "num_epochs = 100\n",
    "best_loss = float('inf')\n",
    "learning_rate = 5e-4\n",
    "weight_dec = 1e-5\n",
    "step_size = 20\n",
    "gamma = 0.1\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_dec)\n",
    "scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    best_loss = checkpoint['loss']\n",
    "    print(f\"Loaded checkpoint, loss: {best_loss:.6f}\")\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(0, num_epochs):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0\n",
    "\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.6f}, best loss: {best_loss:.6f}, time: {(time.time()-start_time):.3f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "        }, checkpoint_path)\n",
    "        print(\"Checkpoint saved.\")\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "378d9aa5-3257-4b61-9ba8-774d9e0ea18b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 1) (3365275964.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[8], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    checkpoint_path = os.path.join(DATA_DIR, 'ecg_checkpoint_fupgcnn_hcrnet_6_6\u001b[0m\n\u001b[0m                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 1)\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = os.path.join(DATA_DIR, 'ecg_checkpoint_fupgcnn_hcrnet_6_6.pth')\n",
    "start_epoch = 0\n",
    "num_epochs = 0\n",
    "learning_rate = 5e-7\n",
    "weight_dec = 1e-5\n",
    "step_size = 10\n",
    "gamma = 0.1\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_dec)\n",
    "scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    best_loss = checkpoint['loss']\n",
    "    print(f\"Loaded checkpoint, loss: {best_loss:.6f}\")\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(0, num_epochs):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0\n",
    "\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.6f}, best loss: {best_loss:.6f}, time: {(time.time()-start_time):.3f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "        }, checkpoint_path)\n",
    "        print(\"Checkpoint saved.\")\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32b0dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    ")\n",
    "\n",
    "# print(\"Class distribution:\", Counter(y))\n",
    "# class_totals = Counter(y)\n",
    "# print(label_map)\n",
    "\n",
    "#https://pmc.ncbi.nlm.nih.gov/articles/PMC4897569/\n",
    "def label_to_name(label):\n",
    "    if label == 'N':\n",
    "        return 'Normal beat'\n",
    "    elif label == 'S':\n",
    "        return 'Supraventricular ectopic beat'\n",
    "    elif label == 'V':\n",
    "        return 'Ventricular ectopic beat'\n",
    "    elif label == 'F':\n",
    "        return 'Fusion beat'\n",
    "    elif label == 'Q':\n",
    "        return 'Unknown beat'\n",
    "\n",
    "def evaluate_model(model, test_loader, label_map, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    index_to_label = {v: k for k, v in label_map.items()}\n",
    "    target_names = [f\"{index_to_label[i]}: {label_to_name(index_to_label[i])}\" for i in range(len(label_map))]\n",
    "\n",
    "    # Metrics\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average=None, zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average=None, zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, average=None, zero_division=0)\n",
    "\n",
    "    # Per-class accuracy\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    per_class_acc = cm.diagonal() / cm.sum(axis=1)\n",
    "\n",
    "    print(f\"\\nOverall Test Accuracy: {acc * 100:.3f}%\\n\")\n",
    "\n",
    "    print(\"Per-Class Metrics:\")\n",
    "    for i, label in enumerate(target_names):\n",
    "        print(f\"{label}\")\n",
    "        print(f\"  Accuracy:                  {per_class_acc[i] * 100:.3f}%\")\n",
    "        print(f\"  Sensitivity (Recall):     {recall[i] * 100:.3f}%\")\n",
    "        print(f\"  Positive Predictivity:    {precision[i] * 100:.3f}%\")\n",
    "        print(f\"  F1 Score:                 {f1[i] * 100:.3f}%\\n\")\n",
    "\n",
    "    # Confusion Matrix Plot\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[index_to_label[i] for i in range(len(label_map))])\n",
    "    disp.plot(cmap=plt.cm.Blues, xticks_rotation=45)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    # Normalized Confusion Matrix\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm_normalized, display_labels=[index_to_label[i] for i in range(len(label_map))])\n",
    "    disp.plot(cmap=plt.cm.Blues, xticks_rotation=45)\n",
    "    plt.title(\"Normalized Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    return acc, all_preds, all_labels\n",
    "\n",
    "\n",
    "acc, preds, labels = evaluate_model(model, test_loader, label_map, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3f0730",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
